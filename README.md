# ğŸ” Fake News Detection System

## âš¡ Quick Start

### **ğŸš€ 5-Minute Setup**
```bash
# 1. Clone & Setup
git clone https://github.com/BharathTT/FakeNewsDetectorUsingBert.git
cd FakeNewsDetectorUsingBert
python3 -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt

# 2. Train Models (2-3 minutes)
make baseline

# 3. Run System
make api     # Terminal 1
make web     # Terminal 2 (new terminal)

# 4. Test at http://localhost:5001
```

### **ğŸ§ª Quick Test**
1. Go to http://localhost:5001
2. Select "Hybrid" model from dropdown
3. Enter: "Scientists found aliens on Mars"
4. Click "ğŸ” Analyze Statement"
5. Should show: âŒ **Fake** (85% confidence)

### **ğŸ”§ Test Files & Commands**
```bash
# Test scraper functionality
python3 debug_scraper.py
# Enter URL when prompted: https://www.bbc.com/news

# Test API directly
curl -X POST "http://localhost:8000/predict" \
     -H "Content-Type: application/json" \
     -d '{"text": "Breaking news from Mars", "model": "hybrid"}'

# Test URL analysis
curl -X POST "http://localhost:8000/predict_url" \
     -H "Content-Type: application/json" \
     -d '{"url": "https://www.ndtv.com/india-news/...", "model": "auto"}'

# Run all tests
make test
```

### **ğŸ“‹ Test Examples**
- **Real News**: "The stock market closed higher today"
- **Fake News**: "Aliens landed in New York yesterday"
- **Test URLs**: 
  - https://www.bbc.com/news
  - https://www.ndtv.com/india-news/...
  - https://www.businessworld.in/article/...

### **ğŸ³ Docker (Even Easier)**
```bash
make docker  # One command runs everything
# Web: http://localhost:5000 | API: http://localhost:8000
```

---

## Table of Contents

1. [âš¡ Quick Start](#quick-start)
2. [Project Overview](#project-overview)
3. [âœ¨ Features](#features)
3. [ğŸ—ï¸ Architecture](#architecture)
4. [ğŸ“Š Dataset](#dataset)
5. [ğŸš€ Installation & Setup](#installation--setup)
6. [ğŸ’» Usage](#usage)
7. [ğŸ¤– Model Selection](#model-selection)
8. [ğŸŒ Web Interface](#web-interface)
9. [ğŸ“¡ API Reference](#api-reference)
10. [ğŸ³ Deployment](#deployment)
11. [ğŸ”§ Commands Reference](#commands-reference)
12. [ğŸš¨ Troubleshooting](#troubleshooting)
13. [ğŸ“ˆ Performance Metrics](#performance-metrics)
14. [ğŸ”® Future Enhancements](#future-enhancements)
15. [ğŸ¤ Contributing](#contributing)
16. [ğŸ“„ License](#license)

---

## Project Overview

The **Fake News Detection System** is a comprehensive, open-source platform that classifies news statements and articles as **real** or **fake** using advanced NLP models. Built with modern web technologies and AI models, it provides both a beautiful web interface and robust REST API.

**ğŸ¯ Key Objectives:**
- Develop a fully free ML pipeline for fake news detection
- Provide multiple model options (BERT, TF-IDF, Keyword matching)
- Beautiful, responsive web interface with URL scraping capabilities
- RESTful API for integration with other applications
- Easy deployment with Docker support

---

## âœ¨ Features

### ğŸ§  **AI Models**
- **ğŸ¤– Hybrid Model**: BERT embeddings + Random Forest classifier
- **ğŸ“Š Baseline Model**: TF-IDF + Logistic Regression
- **ğŸ”¤ Keyword Model**: Simple pattern matching
- **ğŸ›ï¸ Model Selection**: Choose specific models via dropdown

### ğŸŒ **Web Interface**
- **ğŸ¨ Modern UI**: Beautiful gradient design with animations
- **ğŸ“± Responsive**: Works on desktop, tablet, and mobile
- **ğŸ“ Text Analysis**: Direct text input for fact-checking
- **ğŸ”— URL Analysis**: Automatic web scraping from news URLs
- **ğŸ“– Read More/Less**: Expandable full article view
- **ğŸ’¡ Examples**: Quick-test buttons with sample statements

### ğŸ”§ **Web Scraping**
- **ğŸ“° News Sites**: Supports major news websites (NDTV, BBC, CNN, etc.)
- **ğŸ¤– Multiple Methods**: newspaper3k + BeautifulSoup fallback
- **ğŸ“„ Content Extraction**: Automatic title and article text extraction
- **ğŸ›¡ï¸ Error Handling**: Graceful fallbacks when scraping fails

### ğŸš€ **API & Deployment**
- **âš¡ FastAPI**: High-performance REST API
- **ğŸ“š Auto Documentation**: Swagger UI at `/docs`
- **ğŸ³ Docker Support**: Easy containerized deployment
- **ğŸ”„ Health Checks**: API status monitoring
- **ğŸ“Š Model Info**: Shows which model was used for each prediction

---

## ğŸ—ï¸ Architecture

```plaintext
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Web Interface â”‚    â”‚   FastAPI       â”‚    â”‚   AI Models     â”‚
â”‚   (Flask)       â”‚â”€â”€â”€â–¶â”‚   Backend       â”‚â”€â”€â”€â–¶â”‚   Selection     â”‚
â”‚   - Text Input  â”‚    â”‚   - /predict    â”‚    â”‚   - Hybrid      â”‚
â”‚   - URL Input   â”‚    â”‚   - /predict_urlâ”‚    â”‚   - Baseline    â”‚
â”‚   - Model Selectâ”‚    â”‚   - Web Scraper â”‚    â”‚   - Keyword     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ğŸ“ Project Structure:**
```
FakeNewsDetectorUsingBert/
â”œâ”€â”€ app/                    # ğŸ  Main application
â”‚   â”œâ”€â”€ api.py             # ğŸš€ FastAPI backend server
â”‚   â”œâ”€â”€ web.py             # ğŸŒ Flask web interface
â”‚   â”œâ”€â”€ scraper.py         # ğŸ•·ï¸ Web scraping module
â”‚   â”œâ”€â”€ data.py            # ğŸ“Š Data processing
â”‚   â”œâ”€â”€ train.py           # ğŸ“ Model training
â”‚   â”œâ”€â”€ test.py            # ğŸ§ª Testing & validation
â”‚   â”œâ”€â”€ static/            # ğŸ¨ CSS and assets
â”‚   â”‚   â””â”€â”€ style.css      # ğŸ’… Beautiful styling
â”‚   â””â”€â”€ models/            # ğŸ¤– Trained models (created after training)
â”‚       â”œâ”€â”€ hybrid_model.pkl
â”‚       â”œâ”€â”€ baseline_model.pkl
â”‚       â””â”€â”€ vectorizer.pkl
â”œâ”€â”€ liar_dataset/           # ğŸ“š Dataset files
â”‚   â”œâ”€â”€ train.tsv
â”‚   â”œâ”€â”€ valid.tsv
â”‚   â””â”€â”€ test.tsv
â”œâ”€â”€ requirements.txt        # ğŸ“¦ Python dependencies
â”œâ”€â”€ Makefile               # âš¡ Easy commands
â”œâ”€â”€ docker-compose.yml     # ğŸ³ Docker deployment
â”œâ”€â”€ Dockerfile             # ğŸ“¦ Container config
â”œâ”€â”€ .gitignore             # ğŸš« Git ignore rules
â””â”€â”€ README.md              # ğŸ“– This file
```

---

## ğŸ“Š Dataset

**LIAR Dataset**: Benchmark dataset with 12,836 human-labeled political statements from PolitiFact.

| Attribute | Details |
|-----------|---------|
| **ğŸ“° Source** | PolitiFact fact-checking website |
| **ğŸ“Š Size** | 12,836 statements |
| **ğŸ·ï¸ Labels** | 6-class â†’ Binary (Fake/Real) |
| **ğŸ“ Features** | Statement text, speaker, context |
| **ğŸ“ˆ Splits** | Train: 10,269, Val: 1,284, Test: 1,283 |

---

## ğŸš€ Installation & Setup

### **ğŸ“‹ System Requirements**
- ğŸ Python 3.8+ (3.9+ recommended)
- ğŸ’¾ 4GB+ RAM
- ğŸ’½ 2GB+ free disk space
- ğŸŒ Internet connection (for model downloads)

### **1ï¸âƒ£ Clone Repository**
```bash
git clone https://github.com/BharathTT/FakeNewsDetectorUsingBert.git
cd FakeNewsDetectorUsingBert
```

### **2ï¸âƒ£ Setup Python Environment**

**ğŸ macOS/Linux:**
```bash
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

**ğŸªŸ Windows:**
```cmd
python -m venv venv
venv\Scripts\activate
pip install -r requirements.txt
```

### **3ï¸âƒ£ Setup Dataset**
```bash
# Create dataset directory
mkdir -p liar_dataset

# Download sample data (included in repo)
# For full dataset, visit: https://arxiv.org/abs/1705.00648
```

### **4ï¸âƒ£ Train Models**
```bash
# Train baseline model (2-3 minutes)
make baseline

# Train hybrid model (10-15 minutes)
make hybrid

# Train all models
make train
```

---

## ğŸ’» Usage

### **ğŸš€ Quick Start**

#### **1ï¸âƒ£ Start API Server**
```bash
# Terminal 1: Start API
make api
```
**ğŸŒ API available at:** http://localhost:8000

#### **2ï¸âƒ£ Start Web Interface**
```bash
# Terminal 2: Start Web Interface
make web
```
**ğŸ–¥ï¸ Web interface:** http://localhost:5001

#### **3ï¸âƒ£ Test the System**

**ğŸ“ Text Analysis:**
1. Go to http://localhost:5001
2. Select "ğŸ“ Text Analysis" tab
3. Enter: "Scientists discovered aliens on Mars yesterday"
4. Choose model from dropdown
5. Click "ğŸ” Analyze Statement"

**ğŸ”— URL Analysis:**
1. Select "ğŸ”— URL Analysis" tab
2. Enter: "https://www.bbc.com/news/world-..."
3. Choose model from dropdown
4. Click "ğŸ” Analyze Article"

**ğŸ§ª API Testing:**
```bash
curl -X POST "http://localhost:8000/predict" \
     -H "Content-Type: application/json" \
     -d '{"text": "The moon is made of cheese", "model": "hybrid"}'
```

---

## ğŸ¤– Model Selection

The system offers **4 model options** via dropdown menu:

### **ğŸ¯ Auto (Recommended)**
- Tries Hybrid â†’ Baseline â†’ Keyword
- Best accuracy with fallback support
- **Use when**: You want the best possible results

### **ğŸ§  Hybrid Model**
- BERT embeddings + Random Forest
- **Accuracy**: ~62%
- **Speed**: ~200ms
- **Use when**: Maximum accuracy is needed

### **ğŸ“Š Baseline Model**
- TF-IDF + Logistic Regression
- **Accuracy**: ~60%
- **Speed**: ~50ms
- **Use when**: Fast predictions needed

### **ğŸ”¤ Keyword Model**
- Simple pattern matching
- **Accuracy**: ~55%
- **Speed**: ~10ms
- **Use when**: Ultra-fast screening needed

**ğŸ’¡ Model Selection Examples:**
```bash
# Force hybrid model
curl -X POST "http://localhost:8000/predict" \
     -d '{"text": "Breaking news!", "model": "hybrid"}'

# Force baseline model
curl -X POST "http://localhost:8000/predict" \
     -d '{"text": "Breaking news!", "model": "baseline"}'
```

---

## ğŸŒ Web Interface

### **ğŸ¨ Features**
- **ğŸŒˆ Beautiful Design**: Modern gradient background with animations
- **ğŸ“± Responsive**: Works on all devices
- **ğŸ›ï¸ Model Selection**: Dropdown to choose AI model
- **ğŸ“ Dual Input**: Text analysis + URL analysis tabs
- **ğŸ“– Read More/Less**: Expandable article content
- **ğŸ’¡ Quick Examples**: One-click test buttons
- **ğŸ”„ Loading States**: Smooth loading animations
- **ğŸ¯ Color-coded Results**: Green (Real), Red (Fake), Yellow (Uncertain)

### **ğŸ–±ï¸ How to Use**
1. **Choose Model**: Select from dropdown (Auto/Hybrid/Baseline/Keyword)
2. **Select Tab**: Text Analysis or URL Analysis
3. **Enter Input**: Type text or paste news URL
4. **Analyze**: Click the analyze button
5. **View Results**: See prediction with confidence score
6. **Read More**: Click to expand full article (for URLs)

### **ğŸ“± Mobile Support**
- Responsive design works on phones/tablets
- Touch-friendly buttons and inputs
- Optimized for mobile browsing

---

## ğŸ“¡ API Reference

### **ğŸŒ Base URL**
- **Local**: http://localhost:8000
- **Docker**: http://localhost:8000
- **Docs**: http://localhost:8000/docs

### **ğŸ“‹ Endpoints**

#### **POST /predict** - Text Analysis
Analyze text statements for fake news.

**ğŸ“¤ Request:**
```json
{
  "text": "Scientists discovered aliens on Mars",
  "model": "hybrid"  // optional: "auto", "hybrid", "baseline", "keyword"
}
```

**ğŸ“¥ Response:**
```json
{
  "label": "Fake",
  "confidence": 0.85,
  "model": "hybrid"
}
```

#### **POST /predict_url** - URL Analysis
Analyze news articles from URLs.

**ğŸ“¤ Request:**
```json
{
  "url": "https://www.bbc.com/news/world-...",
  "model": "auto"
}
```

**ğŸ“¥ Response:**
```json
{
  "label": "Real",
  "confidence": 0.78,
  "model": "hybrid",
  "title": "Article Title Here",
  "text_preview": "First 200 characters...",
  "full_text": "Complete article text...",
  "scraper_method": "newspaper3k"
}
```

#### **GET /health** - Health Check
```json
{
  "status": "healthy",
  "baseline_loaded": true,
  "hybrid_loaded": true
}
```

### **ğŸ§ª Testing Examples**

**ğŸ Python:**
```python
import requests

# Test text analysis
response = requests.post('http://localhost:8000/predict', json={
    'text': 'The stock market crashed today',
    'model': 'hybrid'
})
print(response.json())

# Test URL analysis
response = requests.post('http://localhost:8000/predict_url', json={
    'url': 'https://www.ndtv.com/india-news/...',
    'model': 'auto'
})
print(response.json())
```

**ğŸŒ JavaScript:**
```javascript
// Text analysis
fetch('http://localhost:8000/predict', {
  method: 'POST',
  headers: {'Content-Type': 'application/json'},
  body: JSON.stringify({
    text: 'Breaking: Major discovery announced',
    model: 'baseline'
  })
}).then(r => r.json()).then(console.log);
```

---

## ğŸ³ Deployment

### **ğŸš€ Docker (Recommended)**

**Quick Deploy:**
```bash
# Build and start all services
docker-compose up --build

# Or use Makefile
make docker
```

**ğŸŒ Access:**
- **API**: http://localhost:8000
- **Web**: http://localhost:5000
- **Docs**: http://localhost:8000/docs

**ğŸ”§ Production:**
```bash
# Run in background
docker-compose up -d

# View logs
docker-compose logs -f

# Stop services
docker-compose down
```

### **ğŸ’» Manual Deployment**

**Local Development:**
```bash
# Terminal 1: API
make api

# Terminal 2: Web
make web
```

**ğŸŒ Production Server:**
```bash
# Install production server
pip install gunicorn

# Run API
cd app && gunicorn -w 4 -k uvicorn.workers.UvicornWorker api:app --bind 0.0.0.0:8000

# Run Web
cd app && gunicorn -w 2 web:app --bind 0.0.0.0:5000
```

### **â˜ï¸ Cloud Deployment**
- **Render.com**: Connect GitHub, auto-deploy
- **Railway.app**: One-click deploy
- **Heroku**: Use Dockerfile
- **Google Cloud Run**: Serverless containers

---

## ğŸ”§ Commands Reference

### **âš¡ Makefile Commands**
```bash
make install    # ğŸ“¦ Install dependencies
make test      # ğŸ§ª Run all tests
make baseline  # ğŸ“ Train baseline model (2-3 min)
make hybrid    # ğŸ§  Train hybrid model (10-15 min)
make train     # ğŸ“ Train all models
make api       # ğŸš€ Start API server
make web       # ğŸŒ Start web interface
make docker    # ğŸ³ Deploy with Docker
make clean     # ğŸ§¹ Clean cache files
```

### **ğŸ Direct Python Commands**
```bash
# Training
cd app && python train.py                    # Train all models
cd app && python test.py                     # Run tests

# Servers
cd app && uvicorn api:app --reload --port 8000    # API server
cd app && python web.py                           # Web server

# Docker
docker-compose up --build                    # Build and run
docker-compose logs -f                       # View logs
```

---

## ğŸš¨ Troubleshooting

### **ğŸ”§ Common Issues**

#### **ğŸ“¦ Installation Problems**
```bash
# Python not found
python3 -m pip install -r requirements.txt

# Virtual environment issues
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

#### **ğŸš€ Server Issues**
```bash
# Port already in use
lsof -ti:8000 | xargs kill -9
make api

# API not responding
curl http://localhost:8000/health
```

#### **ğŸ¤– Model Issues**
```bash
# Models not found
make train

# Training fails
make baseline  # Try baseline only first
```

#### **ğŸŒ Web Scraping Issues**
```bash
# URL analysis fails
# Try different news URLs
# Check API logs for error messages
```

### **ğŸ©º Quick Diagnostics**
```bash
# Test everything
make test

# Check Python environment
python --version
pip list | grep -E "torch|transformers|sklearn"

# Test API health
curl http://localhost:8000/health

# Check model files
ls -la app/models/
```

---

## ğŸ“ˆ Performance Metrics

| **Metric** | **Hybrid** | **Baseline** | **Keyword** |
|------------|------------|--------------|-------------|
| **ğŸ¯ Accuracy** | ~62% | ~60% | ~55% |
| **âš¡ Speed** | ~200ms | ~50ms | ~10ms |
| **ğŸ’¾ Size** | ~400MB | ~1MB | <1KB |
| **ğŸ§  Complexity** | High | Medium | Low |

**ğŸ“Š Use Cases:**
- **ğŸ¯ High Accuracy**: Use Hybrid model
- **âš¡ Fast Response**: Use Baseline model  
- **ğŸš€ Ultra Fast**: Use Keyword model
- **ğŸ›ï¸ Best Balance**: Use Auto mode

---

## ğŸ”® Future Enhancements

### **ğŸ¤– AI Improvements**
- [ ] RoBERTa/DeBERTa models
- [ ] Ensemble methods
- [ ] Multilingual support
- [ ] Confidence explanations (LIME/SHAP)

### **ğŸŒ Web Features**
- [ ] User accounts and history
- [ ] Batch URL analysis
- [ ] Browser extension
- [ ] Real-time news monitoring
- [ ] Social media integration

### **ğŸ”§ Technical**
- [ ] Model quantization
- [ ] Caching system
- [ ] A/B testing
- [ ] Advanced monitoring
- [ ] Auto-scaling

---

## ğŸ¤ Contributing

### **ğŸš€ How to Contribute**
1. **ğŸ´ Fork** the repository
2. **ğŸŒ¿ Create** feature branch: `git checkout -b feature/amazing-feature`
3. **ğŸ’¾ Commit** changes: `git commit -m 'Add amazing feature'`
4. **ğŸ“¤ Push** to branch: `git push origin feature/amazing-feature`
5. **ğŸ”„ Open** Pull Request

### **ğŸ› ï¸ Development Setup**
```bash
# Clone your fork
git clone https://github.com/your-username/FakeNewsDetectorUsingBert.git
cd FakeNewsDetectorUsingBert

# Setup environment
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt

# Run tests
make test
```

### **ğŸ“ Code Style**
- Follow PEP 8 for Python
- Add docstrings for functions
- Include tests for new features
- Keep functions focused and small

---

## ğŸ“„ License

This project is licensed under the **MIT License** - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- **ğŸ“š LIAR Dataset**: Wang, William Yang. "Liar, liar pants on fire": A new benchmark dataset for fake news detection. ACL 2017.
- **ğŸ¤– BERT Model**: Devlin et al. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. NAACL 2019.
- **ğŸ”§ Hugging Face**: Transformers library
- **ğŸ“° PolitiFact**: Dataset source

---

## ğŸ“ Contact & Support

- **ğŸ› Issues**: [GitHub Issues](https://github.com/BharathTT/FakeNewsDetectorUsingBert/issues)
- **ğŸ“§ Email**: bharath@example.com
- **ğŸ“– Documentation**: This README + code comments
- **ğŸ’¬ Discussions**: GitHub Discussions

---

**â­ Star this repository if you found it helpful!**

**ğŸš€ Ready to detect fake news? Let's get started!**